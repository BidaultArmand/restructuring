import os
import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI  # DeepSeek utilise une interface OpenAI-compatible
from dotenv import load_dotenv

load_dotenv()


# === CONFIG ===
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
INDEX_PATH = "index_debug.faiss"
CHUNKS_PATH = "chunks_debug.json"
TABLES_PATH = "data/all_tables.json"
TOP_K = 10  # R√©duit de 20 √† 10 pour √©viter le d√©passement de contexte


API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not API_KEY:
    raise ValueError("‚ö†Ô∏è DEEPSEEK_API_KEY non d√©finie.")

client = OpenAI(
    api_key=API_KEY,
    base_url="https://api.deepseek.com"  # ‚úÖ pas de /v1 ici
)

LLM_MODEL = "deepseek-chat"

# === INITIALISATION ===
print("üîπ Chargement du mod√®le d'embedding et de l'index...")
model = SentenceTransformer(MODEL_NAME)
index = faiss.read_index(INDEX_PATH)

with open(CHUNKS_PATH, "r", encoding="utf-8") as f:
    chunks = json.load(f)

with open(TABLES_PATH, "r", encoding="utf-8") as f:
    tables = json.load(f)

print(f"‚úÖ Index charg√© ({len(chunks)} chunks, {len(tables)} entr√©es tabulaires)")

# Initialisation du client DeepSeek
client = OpenAI(
    api_key=API_KEY,
    base_url="https://api.deepseek.com/v1"  # endpoint officiel DeepSeek
)

# === FONCTIONS ===
def retrieve(query, top_k=TOP_K):
    """Recherche les chunks les plus proches de la requ√™te."""
    q_emb = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(q_emb, top_k)
    return [chunks[i] for i in indices[0]]

import os

def get_tabular_info(doc_id):
    """Associe les infos tabulaires au document, en tol√©rant les variations de nom."""
    results = []
    doc_id_clean = os.path.splitext(doc_id.lower().strip())[0]
    for t in tables:
        src_clean = os.path.splitext(t.get("source", "").lower().strip())[0]
        if src_clean == doc_id_clean:
            results.append(t)
    return results

def build_context(query):
    """Construit le contexte complet √† envoyer au LLM."""
    results = retrieve(query)
    context = ""
    for r in results:
        context += f"\n---\nüìÑ {r['doc_id']}\n{r['text']}\n"
        tab = get_tabular_info(r["doc_id"])
        if tab:
            context += f"\nüìä Donn√©es tabulaires : {json.dumps(tab, ensure_ascii=False, indent=2)}\n"
    return context.strip()



def ask_deepseek(query, context):
    messages = [
        {
            "role": "system",
            "content": (
                "Tu es un assistant juridique expert en restructuration d'entreprise. "
                "Utilise uniquement les informations du contexte fourni pour r√©pondre pr√©cis√©ment."
            )
        },
        {
            "role": "user",
            "content": f"Contexte :\n{context}\n\nQuestion : {query}"
        }
    ]

    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=messages,
        temperature=0.2,
    )

    return response.choices[0].message.content


# === PIPELINE RAG COMPLET ===
def rag_query(query):
    """Ex√©cute une requ√™te RAG compl√®te."""
    print(f"\nüîç Requ√™te : {query}")
    context = build_context(query)
    print(f"üìö Contexte construit ({len(context)} caract√®res)")
    answer = ask_deepseek(query, context)
    print("\nüß† R√©ponse DeepSeek :\n")
    print(answer)
    return answer

# === TEST ===
if __name__ == "__main__":
    query = "donne moi toutn les d√©bits du compte n¬∞ 23074642462, fais moin le d√©ta√©ils de chaque d√©bits"
    rag_query(query)
